\documentclass{article}
\usepackage{amsmath}
\usepackage{dcj}

\begin{document}

\vspace{8em}

The paper presented here is revised version of the one submitted on Sept. 30
2011. In this last round of review, we found the reviewers comments to be
very constructive overall, and are grateful for their time and attention. Some
small errors were discovered, as well as several points of confusion which we
have taken care to address. While no major changes to the method or results were
required, we have, in a number of ways, improved the clarity of our
presentation, and believe that is now fit for publication in Bioinformatics.

Belew is a summary of the changes made, in addition to specific responses to the
points raised by the reviewers.

\section*{Summary of Changes}

\begin{itemize}
\item In response concerns raised by reviewers 2 and 3, we we have reworded the
title and abstract to make clear that our results are most directly applicable
to RNA-Seq, and we are not yet able to make strong assertions regarding
ChIP-Seq.

\item We have taken the first reviewer's suggestion and have highlighted the
strong theoretical results our method provides in the abstract.

\item The first paragraph of Section 2.1.1 has been reworded to make clear that
our method does ignore duplicate reads when training.

\item We have removed our use of the term ``classification'' in the fifth
paragraph of Section 2.1, as it seemed only to cause confusion.

\item We have clarified protocol differences between the Wetterbom and Katze
data sets in Table 1, in response to the second reviewer's question.

\item We have included new data in Supplementary Section 2 showing that the
results presented are not typically sensitive to the values of $p_{\text{max}}$
and $d_{\text{max}}$

\item The description of the procedure we used to compute KL divergence in the
3rd paragraph of Section 3.1 has been rewritten to be more clear.

\item We have corrected an inconsistency in the scale of figures 1 and 4.

\item We have clarified that the values in Table 3 are Pearson correlation
coefficient $r$, and not $r^2$.

\item We have repeated the RT-PCR analysis of Section 3.3 using version 60 of the
Ensembl gene annotations, rather than version 62, to remain consistent with the
Poisson regression and KL divergence analysis.

\item In the second paragraph of Section 4, we have added the proper citation of
what we believe to be first published derivation of the results of Supplementary
Section 10.

\item In Section 4, we have made note of recent paper by Jayaprakash, et al, as
well as the work by Zheng, et al, that was brought to our attention by the
second reviewer.

\item Table 2 was moved to Supplementary Section 3 to preserve space.

\end{itemize}

\section*{Response to Reviewer 1}

\begin{quote}
In the supplement the authors show an upper bound on the probability that their
method will learn a non-empty model when in fact there is no bias in the data. I
think this is an important feature of the method and should be highlighted
better in the paper.
\end{quote}

We appreciate this suggestion, and made note of the theoretical results in the
revised abstract of the paper.


\begin{quote}
My only concern is with the sampling described on pg3, column 2, lines 32-47.
In RNA-seq data the distribution of read count can span 5-6 orders of
magnitudes, the top expressed gene accounting for up to 10\% of the data
(especially tissue specific). If you sample at random you are selecting more
frequently from the top, because of this heavy skew in the distribution. You
discuss this issue when measuring KL-divergence in section 3.1, but it does not
appear to affect the training. My main worry is that this will bias the
adjustment to mostly highly expressed genes. How do you take care of this?

One approach would be to simply discard duplicates. In that case how do you
count the sequences, i.e. if two reads map to the same position with one with
sequencing errors, one without do you count them as two sequences or do you
treat them as one and use the sequence of the genome?
\end{quote}

In fact, we do ignore duplicate reads when training the method. This was stated
in the first paragraph of Section 2.2.1 (``we take sequences surrounding
(extending 20 nt to either side, by default) the start positions of a randomly
sampled set of $n/2$ aligned reads, ignoring duplicate reads''), but the reviewer is
correct that this is an important detail, as the method might otherwise be
overfit to a few highly expressed genes. In this revision we have expanded
this paragraph to clarify this decision.


\begin{quote}
pg 3, column 1, lines 42-50 \\
Although BN's are mostly used for classification purposes they are just a
compact representation of a discrete probability distribution, the discussion
of classification is somewhat confusing.
\end{quote}

It was our intention to point out that the training algorithm used (i.e.,
maximizing the conditional log-likelihood) is typical of classification
applications, but this point was perhaps needlessly confusing. We have
rewritten the paragraph in question (the fifth paragraph in Section 2.1,
beginning with ``Towards that end...'') to avoid this.


\begin{quote}
supplement pg 7, figure 11. \\
Since you have the 4 replicates from the Trapnell et al. data and you show the
nucleotide frequencies for each replicate, why not run your method independently
on each replicate and compare the modified results.
\end{quote}

We agree that a looking for an increase in correlation of gene abundance
estimates between replicates after bias correction would be an interesting
analysis. Unfortunately, the four RNA-Seq data sets from Trapnell, et al, were
taken from four different biological states and compared without technical or
biological replicates, so a comparison of gene abundance estimates would be
conflated with genuine changes in gene expression.  While we wish to draw
attention to the potential of batch effects in RNA-Seq, a larger survey of
variability of technical bias is outside the intended scope of the paper.


\section*{Response to Reviewer 2}

\begin{quote}
General concerns. This work has an underlying assumption that base-level read
counts are biased and mainly affected by surrounding nucleotide frequencies.
For RNA-Seq data, this assumption has been justified by previous work (Hansen et
al. 2010 and Li et al. 2010). But it is not readily extendable to other NGS
data, such as ChIP-Seq. ChIP-Seq has an enrichment step for DNA fragments with
very specific characteristics (e.g. transcription factor binding sites), it is
imaginable that the surrounding nucleotide frequencies around a ChIP-Seq peak
may be quite different from other genomic regions, and most sequencing reads
from real binding sites may have very similar surrounding nucleotide
frequencies. To accurately measure read counts for a ChIP-Seq peak, it matters
more to distinguish reads from background and from real binding sites and
correct for protocol-related biases such as PCR over-amplification. The authors
may need more effort to establish the fact that biases related to surrounding
nucleotide frequencies exist in multiple ChIP-Seq data sets.
\end{quote}

The reviewer is correct that we have by no means established that applying our
method to ChIP-Seq leads to more accurate results, and have instead concentrated
on making this case for RNA-Seq. We have included some very preliminary ChIP-Seq
results, only to suggest that investigating the effects of sequence bias in
ChIP-Seq is a promising future research direction. To avoid the impression that
we are making an overly-broad claim, we have reworded the title and abstract to
reflect that our results relate most readily to RNA-Seq.


\begin{quote}
Moreover, even for RNA-Seq data, only considering biases in base-level read
counts does not seem to be enough for accurate quantitation of transcript level
as shown in (Zheng et al. 2011 BMC Bioinformatics). Features other than the 20
upstream and downstream nucleotides surrounding read starts may also cause
biases in gene/transcript quantitation. The authors may discuss a little more on
their assumptions.
\end{quote}

The Zheng paper makes the assertion that ``the bias patterns across genes were
not adequately addressed by the base-level bias correction methods, and GAM
[Generalized Additive Model] correction is necessary to further remove these
biases.'' This follows from their observation that Hansen's heptamer reweighting
method did not improve correlation bewteen RNA-Seq and RT-PCR. Indeed, our
results are mostly consistent, showing only a slight improvment.

However, the authors apparently did not evaluate this correlation after applying
a more sophisticated model such as the GLM or MART models as proposed by Li. As
we demonstrate in Section 3.3, applying a more flexible model results in a large
increase in correlation, suggesting such an approach should not be hastily
dismissed.


\begin{quote}
Page 2, Figure 1. Wetterbom data does not seem to have much bias in surrounding
nucleotide frequencies. What is the difference between this and Katze data set
that makes such difference? Why do the different Illumina data sets have similar
bias trends whereas ABI data sets do not?
\end{quote}

While both these data sets were generated using the ABI platform, there are
significant differences in the protocol used. Most significantly, the Wetterbom
data was generated using poly-A primers to enrich for mature, polyadenylated
mRNA. In the Katze data, ribosomal RNA was depleted, but polyadenylated
transcripts were not selected for, in an effort to sequence more of the
non-polyadenylated transcriptome. The Katze data set is unique in this aspect:
all three of the Illumina data sets were generated with mRNA-Seq protocols as
well.  We have revised Table 1 to make note of this fact.


\begin{quote}
Page 4, line 31, left column. How are the default values of pmax and dmax
determined? Could you provide a table with model complexity, model performance
and running time with different pmax and dmax choices?
\end{quote}

As suggested by Figure 3 in the paper, most edges added to the model are between
adjacent nucleotides, and the in-degree of most positions is small.  We have now
included additional data (Figures 4 and 5 in the Supplement) showing that the
performance of the model on the Mortazavi data set is not greatly effected by
these parameters as long as they are both non-zero. It should not be assumed
that this is necessarily the case with every data set, and so these parameters
default to somewhat arbitrary larger numbers that balance training time with
model flexibility.


\begin{quote}
Page 5, line28, right column. This sentence is confusing. Are the duplicated
reads counted or not? Is ``upper half'' referring to the position in the exon or
reads with number of duplicates in the upper 50 percentile?
\end{quote}

We have rewritten the 3rd paragraph in Section 3.1 to provide a more lucid
description of the procedure.


\begin{quote}
Page 6, figure 4. Is the unadjusted k=1 case equivalent to the KL panel in
Figure 1? If so, why is Bullard data not consistent KL=0.6 in figure 4 and 0.3
in Figure 1. Other data sets seem to be consistent.
\end{quote}

Regrettably, in Figure 1, the natural logarithm was used to compute KL
divergence, while in Figure 4, the base 2 logarithm, causing the scale to
somewhat shifted between the two. This error has been corrected in the revision.


\begin{quote}
Page 7, table 4. Is the presented correlation $r$ or $r^2$?
\end{quote}

The correlation measurement used was the Pearson's $r$, which he have clarified.


\begin{quote}
Different versions of annotation were referred. Page 4 line 49: Ensembl 60 is
used in training models, page 7 line 32, Ensembl 62 is used in comparing to
RT-PCR data. Should make these consistent.
\end{quote}

We have repeated tho RT-PCR analysis with Ensembl 60, to remain consistent with
the Poisson regression and KL divergence results.


\section*{Response to Reviewer 3}


\begin{quote}
The result sections, however, did not include any comparison directly
on bias reduction. Rather, results are focused on improvement of model
fitting and statistical significance of correlation -- which are both
important but not the ultimate goal.

In terms of bias reduction: first, it is necessary to see the amount of bias
reduced using the proposed approach versus the other methods in
comparison.  The increase in uniformaity is a promising sign, but does
not guarantee that the gene expression measure is improved.  With the
bias correction implemented, what will be the recommended measure for
a gene or exon level expression? Sum of $\Pr[m_i|s_i]$? Whether the bias of
this value is reduced is a much more convincing argument than the
increased uniformity itself. There are  biological information that
would allow this type of comparison. We have seen only one example in
Fig 2, but no direct quantification in bias reduction.
\end{quote}

We use bias to refer to the fact that the estimator in question (RNA-Seq
based measurement of gene abundance) does not, in expectation, agree with the
value we wish to estimate (the true gene abundance). To measure the bias
directly, we would then need to know the true number of copies of each
transcript we wish to quantify and compare RNA-Seq based estimates before and
after correcting for sequence bias. Because exact quantification of transcript
copy number is not feasible at a large scale with any existing technology, we
instead used the largest publically available RT-PCR data set that we are aware
of and demonstrated a substantial increase in the agreement between RNA-Seq and
RT-PCR after correcting for bias in the former (see Section 3.3).

In the example shown in Figure 2, we provided a qualitative illustration of the
large scale quantitative analysis of Section 3.1 and Section 3.2, demonstrating
a reduction in the symptoms of sequence bias across thousands of exons in five
data sets, and in comparison to three previously proposed methods. In sum, we
believe this represents the most complete and rigorous characterization of
sequence bias in RNA-Seq yet presented. It is a compelling convergence of
evidence that the accuracy of RNA-Seq is improved by correcting for sequence
bias, and that the method we propose is particularly suited to do so.


\begin{quote}
Second, as the authors point out in introduction, another problem of
these biases is that they cannot be assumed to be ``identical between
replicates". How much of the non-identical bias is removed with the
proposed approach?
\end{quote}

We have seen that sequence bias is not always consistent, even in experiments
using similar protocols (see Figure 1, for example), and believe that batch
effects in RNA-Seq are a serious concern. So far, we have concentrated only on
rigorously demonstrating an absolute reduction in sequence bias, and have not
undertaken any systematic comparison of the consistency of this bias between
replicates. Such comparison would be meaningful, but outside of the intended
scope of the paper.


\begin{quote}
Another issue as important as bias reduction is variance reduction. The
bias-variance trade off is unavoidable. By estimating the bias and trying to
adjust for it, more variance is introduced to the system. Is the amount of bias
reduced worth the increased variance, for downstream analysis?
\end{quote}

We agree that it possible to reduce bias and yet introduce so much variance that
that experiments are actually rendered less reliable. In the paper, we attempted
to capture this tradeoff most directly with Figure 5, which demonstrates that,
while uniformity of read coverage is increased ($R^2 > 0$) in the large majority
of exons after applying our method, inevitably it is decreased ($R^2 < 0$) in
some. This suggests that the (BIC) complexity penalty we use during model
selection works as desired, and as its ansymptotic properties predict,
controlling variance by preventing very dense models from being trained.


\begin{quote}
A minor point: If all examples are from RNAseq experiment, how should
one modify the method when it is applied to other sequencing data?
Would one do the same for ChIP-Seq? IF the current method is meant for
RNAseq only, the title should reflect this instead of general ``high
throughput sequencing data".
\end{quote}

No modification at all is needed to apply the method to ChIP-Seq data. This is
true also of the Hansen method, but not the case with methods of Li or Roberts.
While we believe application to ChIP-Seq is a promising research direction and
one that our method is particularly suited for, besides some preliminary
analysis the results presented here are directed at RNA-Seq. We have
reworded the title and abstract to reflect this.

\end{document}


