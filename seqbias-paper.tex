
\documentclass{bioinfo}
\copyrightyear{2011}
\pubyear{2011}

\begin{document}
\firstpage{1}

%\title{Correcting for bias in high-throughput sequencing data}
\title{A new approach to bias correction in RNA-Seq}
\author[Jones \textit{et~al}]
{Daniel C. Jones\,$^{1,}$
\hspace{-0.5em}\footnote{to whom correspondence should be addressed}\hspace{0.5em},
Walter L. Ruzzo\,$^{1,2,3}$,
Xinxia Peng$^{4}$
and
Michael G. Katze$^{4}$
}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\address{
$^{1}$Department of Computer Science and Engineering, University of
Washington, Seattle, WA 98195-2350, USA\\
$^{2}$Department of Genome Sciences, University of Washington, Seattle, Wa
98195-5065, USA\\
$^{3}$Fred Hutchinson Cancer Research Center, Seattle, WA 98109, USA\\
$^{4}$Department of Microbiology, University of Washington, Seattle, WA
98195-7242, USA}

\editor{Associate Editor: XXXXXXX}

\maketitle

\begin{abstract}

\section{Motivation:}
Quantification of sequence abundance in RNA-Seq experiments is often conflated
by protocol-specific sequence bias. The exact sources of the bias are unknown,
but may be influenced by PCR amplification, or differing primer affinities and
mixtures, for example. The result is decreased accuracy in many applications,
such as de novo gene annotation and transcript quantification.

\section{Results:}
We present a new method to measure and correct for these influences using a
simple graphical model. Our model does not rely on existing gene annotations,
and model selection is performed automatically making it applicable with few
assumptions. We evaluate our method on several data sets, and by multiple
criteria, demonstrating that it effectively decreases bias and increases
uniformity. Additionally, we provide theoretical and empirical results showing
that the method is unlikely to have any effect on unbiased data, suggesting it
can be applied with little risk of spurious adjustment.


\section{Availability:}
The method is implemented in the \texttt{seqbias} R/Bioconductor package,
available freely under the LGPL license from
\href{http://bioconductor.org}{http://bioconductor.org}.


\section{Contact:}
\href{dcjones@cs.washington.edu}{dcjones@cs.washington.edu}

\end{abstract}


\section{Introduction}

In the last few years, RNA-Seq has emerged as a promising alternative to
microarrays in quantifying RNA abundance. But, as microarray technology has
brought with it technical challenges ranging from developing robust
normalization to accounting for cross-hybridization, RNA-Seq presents a new set
of challenges. As first noted by \citet{Dohm2008}, a particular challenge is the
often complex and protocol-specific influence of nucleotide sequence on
quantification.

In an ideal experiment, the number of RNA-Seq reads mapping to a particular
position in the genome is a function of RNA abundance and should not be
additionally dependent on the sequence at that position. Yet, this is not the
case. As illustration, Figure \ref{fig:freqs} plots this nonuniformity in
nucleotide frequencies on five data sets (see Table \ref{tab:datasets}), each
using a different protocol.

These biases may adversely effect transcript discovery, as low level noise may
be over-reported in some regions, and in others, active transcription may be
under-reported. They render untrustworthy comparisons of relative abundance
between genes or isoforms, and any test of differential expression hangs on the
assumption that these biases are identical between replicates, an undesirable
assumption given that the causes of the bias are not well understood.
Additionally, in many tests of differential expression higher read count will
result in higher statistical confidence. It follows that the sensitivity of such
a test will also be biased by sequence, affecting downstream analysis such as
gene ontology enrichment tests.

This bias, though observed primarily in the $5'$ end of a read, is not resolved by
trimming the reads prior to mapping \citep{Hansen2010} (see also Supplementary
Section 1), suggesting it is not a result of erroneous base calling, and that a
more sophisticated means of correction is needed.

\begin{figure*}
\centerline{\includegraphics[width=0.85\textwidth]{freqs.eps}}
\caption{Nucleotide frequencies are plotted relative to the start (labeled
position 0) of each mapped read, respecting strand, and grouped by platform
(Illumina or ABI SOLiD).  The data sets plotted here are those used
for evaluation, listed in Table \ref{tab:datasets}.
The sequence is taken from the genomic context surrounding the read, so that -40
to -1, for example, fall outside the read sequence itself. The symmetrized
Kullback-Leibler divergence is used to summarize the difference in nucleotide
frequency compared to a fixed estimate of background nucleotide frequencies made
by sampling many positions near mapped reads.  Under the assumption that reads
are sampled uniformly from transcripts, each of the plots should be essentially
flat.}
\label{fig:freqs}
\end{figure*}



\citet{Li2010} propose two models. The first is a Poisson linear model, in which
read counts across a transcript follow an inhomogeneous Poisson process. The
read count at position $i$ within the transcript is Poisson distributed with
parameter $\lambda_i$, where, $\log(\lambda_i)$ is the sum of independent
weights determined by the nucleotide at each position surrounding the read
start, in addition to a term capturing the abundance of the transcript.

The second model is based on multiple additive regression trees, or MART
\citep{Friedman2003}.  In their tests, the MART model shows a moderate
improvement over the Poisson linear model. Both models are fit to a number of
abundant test genes, requiring existing gene annotations for the reference
genome. 

Another model, proposed by \citet{Hansen2010}, directly estimates the
distribution of initial heptamers within reads, then estimates a presumed
background heptamer distribution, sampled from the ends of reads. The read count
at a given position is then adjusted by the ratio of the foreground and
background heptamer probabilities. Specifying two distributions over heptamers
(i.e., foreground and background distributions) requires \mbox{$2(4^7-1) =
32,766$} parameters, so while no gene annotations are needed to train such a
model, a significant number of reads are required, and a number that increases
exponentially with $k$, if it were desirable to model $k$-mers for $k > 7$.

Lastly, \citet{Roberts2011} have very recently published a description of
another approach, in which sequence probabilities are modeled by variable-order
Markov chains. The structure of these Markov chains are hard-coded, chosen in
advance using a hill-climbing algorithm on a representative data set. This method
is implemented in the latest version of Cufflinks \citep{Trapnell2010}, and
tightly incorporated into its estimation of transcript abundance, requiring
either predicted or existing gene annotations.

Here we propose a new approach, using Bayesian networks to model sequence
probabilities. Unlike the methods of Roberts or Li, our model requires no gene
annotations, nor even the assumption that the short reads are derived from RNA.
In this sense, we build on the work done by \citet{Hansen2010}, generalizing
their approach in a way we find to be more robust and effective at correcting
for bias in a variety of protocols. Due to the weak assumptions required by our
model, it is applicable and potentially useful in any setting in which short
reads are aligned to a reference sequence.

\section{Methods}

\subsection{Principle}

We begin with a natural model of an RNA-Seq experiment (and one that is often
assumed, whether implicitly or otherwise). The number of reads $x_i$ aligned to
genomic position $i$ is an unbiased estimate of RNA abundance. Furthermore, we
assume reads may be treated as independent and identically distributed samples.
That is, if $N$ reads are generated, and $m_i$ is the event that a generated
read maps to position $i$, then $ E[x_i] = N \Pr[m_i] $.

The experiment may be considered unbiased with regards to sequence if, having
observed the nucleotide sequence $s_i$ surrounding position $i$, the expected
number of reads sampled from position $i$ is independent of $s_i$, i.e., if
$$ E[ x_i | s_i ] = N \Pr[ m_i | s_i ] = N \Pr[ m_i ] = E[ x_i ] $$
From Bayes' rule,
$$ \Pr[ m_i | s_i ] = \frac{ \Pr[ s_i | m_i ] \Pr[ m_i ] }{ \Pr[ s_i ] } $$
This suggests a natural scheme in which observations may be reweighted to
correct for bias.  First, define the \emph{sequence bias} $b_i$ at position $i$
as $ b_i = \Pr[ s_i ] /  \Pr[ s_i | m_i ]  $.

Now, if we reweight the read count $x_i$ at position $i$ by $b_i$, we
have,
\begin{align*}
E[ b_i x_i | s_i ] &= b_i E[ x_i | s_i ] \\
&= N b_i \Pr[ m_i | s_i ] \\
&= N \frac{ \Pr[ m_i | s_i ] \Pr[ s_i ] }{ \Pr[ s_i | m_i ] } \\
&= N \Pr[ m_i ] \\
&= E[ x_i ]
\end{align*}
Thus the reweighted read counts are made unbiased.

To estimate the bias $b_i$, we must make estimates of the background sequence
probability $\Pr[s_i]$ and the foreground sequence probability $\Pr[ s_i | m_i
]$, the latter being the probability of the sequence given a read being sampled
from its position. Estimating bias is therefore a problem of finding a model of
sequence probability that is sufficiently complex to capture the common features
of the training data yet avoids overfitting.

Towards that end, we propose training a Bayesian network using examples of
foreground and background sequences. By training the model discriminatively and
penalizing model complexity, we can avoid a model that is over-parametrized,
excluding parameters that are insufficiently informative in discriminating
between foreground and background. The Bayesian network can then be used to
evaluate sequence probability, and thus bias, at any genomic position. Figure
\ref{fig:overview} gives a schematic overview of the proposed model.

\begin{figure}
\centerline{\includegraphics[width=0.37\textwidth]{overview.eps}}
\caption{An overview of the approach taken: (a) foreground sequences are sampled
from the regions surrounding the starts of mapped reads, (b) background
sequences are sampled by randomly offsetting foreground positions, (c) a
Bayesian network is trained to discriminate between the set of
sampled foreground and background sequences, (d) and the model is evaluated
at each position within a locus, predicting bias. The predicted bias can then be
used to adjust read counts, as in (e). In (d) and (e) we show the results of
this method applied to the $3'$ UTR of Apoa2, using data from
\citet{Mortazavi2008}. In bias coefficients predicted across 10 million positions
of chromosome 1, the log10 bias of 95\% of the positions were between -1.14 and
0.63, suggesting that most adjustments are not large. The $R^2$ measure,
detailed in Section 3.2, gives the relative increase in log-likeliood under a
uniform sampling model, after correcting for bias, with 1.0 indicating a perfect
fit, and the score of 0.38 here indicating a significant increase.}
\label{fig:overview}
\end{figure}

We have so far ignored one complication: the RNA abundance that we wish
to estimate is not itself independent of the nucleotide sequence. Notably,
exonic DNA tends to be more GC-rich than intergenic DNA. If background sequences
are sampled uniformly from the genome we run the risk of incorrectly adjusting
for biological sequence bias, rather than technical sequence bias.  To avoid
this, we propose using paired training data. Each foreground training
sequence is paired with a background sequence taken from a nearby position
that is likely to have similar abundance and general nucleotide composition.
Alternatively, we could pair foreground samples with background samples from
within the same transcript, but we prefer to avoid dependence on existing
gene annotations.

The methods proposed by \citet{Hansen2010} and \cite{Roberts2011} also treat
bias correction as a problem of estimating foreground and background sequence
probabilities. They differ primarily in how these sequence probabilities are
estimated. \citet{Li2010} estimate reweighting coefficients ($b_i$, in our
notation) directly, given training data consisting of long annotated, highly
expressed transcripts.



\subsection{Estimation}

To estimate sequencing bias, we train a Bayesian network in which each node
represents a position in the sequence, relative to the read start, and edges
encode dependency between positions.  Bayesian networks have been applied to
recognize motifs in nucleotide sequences in the past, in particular in modeling
splice sites \citep{Cai2000, Chen2005} and transcription factor binding sites
\citep{Ben-Gal2005, Grau2006, Pudimat2005}. 

In our model, we do not rely on constraining the set of networks (e.g., to
trees), and instead approximate the NP-Hard problem of determining the optimal
network structure using a fast hill-climbing algorithm. Furthermore, we
train our model \emph{discriminatively}; only parameters that are deemed
informative in discriminating between foreground and background sequences are
included in the model. We thus seek to train a model that reduces bias, without
including uninformative parameters that would only increase variance.

\subsubsection{Sampling}
The model is trained on $n$ sequences, one half labeled as foreground, the other
background, sampled from the reference genome. To obtain the foreground
sequences, we take sequences surrounding (extending 20 nt to either side, by
default) the start positions of a randomly sampled set of $n/2$ aligned reads.
To avoid the risk of the method being overfit to reads deriving from a few
highly expressed genes, we ignore duplicate reads, which we define as two reads
mapping to the same location in the genome. The nucleotide sequence is taken
from the genome, rather than the reads themselves, allowing us to include
positions outside of the read.

To obtain background training sequences, we randomly offset the positions from
which the foreground sequences were sampled.  The offset is drawn from a
zero-mean Gaussian (with $\sigma^2 = 10$, by default), and rounded to the
nearest integer, away from zero.  By using such a scheme, we attempt to mitigate
the effects of biological sequence bias, sampling positions that are more likely
to be biologically similar.

This procedure produces a training set of $n$ sequences with accompanying labels
$T = \{ (s_1, x_1), (s_2, x_2), \dots, (s_n, x_n) \}$. The label $x_i$ is
binary, indicating classification as background ($x_i = 0$) or foreground ($x_i
= 1$).

\subsubsection{Training}
To determine the structure and parameters of the Bayesian network, we use a
hill-climbing approach similar to the algorithm described by \citet{Grossman2004}.
The network structure is determined by greedily optimizing the conditional
log-likelihood:
\begin{align*}
\ell &= \sum_{i=1}^{n} \log \Pr[ x_i | s_i ]
=
\sum_{i=1}^{n} \log \frac{ \Pr[s_i | x_i] \Pr[ x_i ] }{
\sum_{x \in \{0,1\}} \Pr[ s_i | x ] \Pr[x] } \\
\end{align*}
where $\Pr[x]$ is flat (i.e.  $\Pr[ x = 0 ] = \Pr[ x = 1 ] =
0.5$) since we sample foreground and background positions equally.

As we will be estimating parameters and evaluating the likelihood on the same
set of samples, simply maximizing the likelihood would severely overfit the
training set. We thus penalize model complexity heuristically using the Bayesian
Information Criterion \citep{Schwarz1978a}. Where $m$ is the number of
parameters needed to specify the model, we maximize, 
$ \ell' = 2 \ell - m \log n $.

Some benefit might be obtained from a more highly tuned complexity penalty.
However, since the model is trained greedily, additional parameters will be
decreasingly informative, and increasingly similar between foreground and
background. Adding more parameters will have little effect.  Only when $m$ is
allowed to grow exponentially does the prediction become polluted by small
deviations between thousands of uninformative parameters. 

At each step of the optimization procedure, every possible edge or position
addition, removal, or edge reversal that produces a valid, acyclic network
is evaluated, and the alteration that increases the score
$\ell'$ the most is kept.  This process is repeated until a local maximum is
found, in which no single alteration to the network will increase the score.
Given the network structure, the parameters are estimated directly from the
observed nucleotide frequencies in the training data.

The run time of the training procedure is further reduced in practice by
imposing the following two restrictions on the structure of the network, First,
the in-degree (i.e., number of parents) of any node must be less than some
number $p_{\text{max}}$. Secondly, for all edges $(i,j)$, $|j - i| \le
d_{\text{max}}$ for some number $d_{\text{max}}$. This latter rule encodes the
assumption that distant nucleotides are effectively independent. We choose
$p_{\text{max}} = 4$ and $d_{\text{max}} = 10$, as reasonable default values
(see Supplementary Section 2).

Figure \ref{fig:models} shows examples of the structure learned when this
procedure is applied to several data sets, using 100,000 reads from each.


\section{Results}

\begin{figure*}
\centerline{\includegraphics[width=\textwidth]{models.eps}}
\caption{
The network structures learned on each of the data sets are displayed. Positions
are relative to the read start, which is labeled 0. Hollow circles indicate
positions that were not included in the model, being deemed uninformative, given
the other positions and edges. The number of parameters needed to
specify each model is listed in parenthesis below. Applied to data with less
bias, a sparser model is trained, as evinced by the Wetterbom data set. Note
that dependencies (i.e., arrows) tend to span a short distances, and nodes tend
to have a small in-degree (i.e., have few inward arrows). In practice, we save
time in training by prohibiting very distant dependencies (greater than 10, by
default) or very high in-degrees (greater than 4, by default).
}
\label{fig:models}
\end{figure*}


Because we cannot observe directly the underlying RNA-abundance, our evaluation
strategy relies on testing three assumptions we make of an ideal, unbiased
RNA-Seq experiment.
\begin{enumerate}
\item Positional nucleotide frequencies (as in Figure \ref{fig:freqs}), measured
from reads within exons, should not differ greatly from frequencies measured by
sampling uniformly within the same exons.
\item Read counts across a single exon should follow, approximately, a Poisson
process.
\item Adjusting for bias in RNA-Seq should increase the agreement between RNA-Seq and
an other methods of quantification.
\end{enumerate}

Evident from Figure \ref{fig:overview}, the assumption of uniform read coverage
often does not hold in typical RNA-Seq data sets. Although the bias corrected
read counts across the exon pictured in this example are visibly more uniform,
we sought a simple, objective tests that could be applied genome-wide. To this
end, we used cross-validation tests (i.e., methods were trained and tested on
disjoint subsets of the same RNA-Seq data sets) of a quantitative measure of the
increase in uniformity of nucleotide frequencies (Kullback-Leibler divergence in
Section 3.1) and increase in uniformity of read coverage (Poisson regression in
Section 3.2). Additionally, we compare RNA-Seq based estimate of gene expression
to qRT-PCR based estimates for the same genes, showing increased correlation
between the two methods after bias correction (Section 3.3).

To evaluate the first two assumption, we applied our procedure (labeled ``BN'')
as well as those of \citet{Li2010} (``GLM'' and ``MART'') and \citet{Hansen2010}
(``7mer''), which are implemented in the R packages \texttt{mseq} and
\texttt{Genominator}, respectively, to four publicly available data sets
\citep{Bullard2010, Mortazavi2008, Trapnell2010, Wetterbom2010}, as well as an
unpublished data set of our own (Table \ref{tab:datasets}).

\begin{table}
\processtable{Data sets on which the methods are evaluated.\label{tab:datasets}}
{
\begin{tabular}{lllcc}\toprule
                      &            &          &          & Read  \\
Experiment            & Species    & Platform & Protocol & Length \\\midrule
\citet{Wetterbom2010} & Chimp.     & ABI      & mRNA     & 33  \\
Katze (unpublished)   & Macaque    & ABI      & WT       & 50 \\
\citet{Bullard2010}   & Human      & Illumina & mRNA     & 35 \\
\citet{Mortazavi2008} & Mouse      & Illumina & mRNA     & 33 \\
\citet{Trapnell2010}  & Mouse      & Illumina & mRNA     & 75 \\\botrule
\end{tabular}
}{
The protocol column lists whether a poly-A priming step to select for
polyadenylated transcripts was used (``mRNA''), or depletion of ribosomal RNA
with no step to select for polyadenylated transcripts (``WT'').
}
\end{table}


Each method was trained on data taken
from chromosomes 1--8 of the genome from which the reads were mapped (including
chromosomes 2a and 2b of the Chimpanzee genome). For evaluation, we drew a set
of long, highly expressed exons from the remaining chromosomes. In particular,
for each reference sequence, beginning with the set of exons annotated by
Ensembl release 60 \cite{Hubbard2009}, we removed any exons with known alternate
splice sites, then chose the top 1000 exons by read count, restricting ourselves
to those at least 100 nt long.

The differences in the methods being tested necessitated training procedures
unique to each. The total number of reads used to train each method is listed in
Supplementary Section 3, and below we describe the procedure used for
each.

\citet{Li2010} recommends that their MART and GLM models be trained using the
100 most abundant genes. We used 1000 exons from chromosomes 1--8, otherwise
chosen in a manner identical to that which was used to select the test exons.
Both the GLM and MART models were trained considering the initial read position
and 20 nucleotides upstream and downsream, and otherwise using default
parameters.

\citet{Hansen2010} recommends using all of the reads to estimate heptamer
frequencies used by their model. The training procedure works by simple tallying
of frequencies. The implementation of this model in the Genominator package
uses a great deal of memory, and we were unable to train with the volume of
data we wished, so we reimplemented the model and trained it on all of the reads
aligned to chromosomes 1--8.

We evaluated several variations of the heptamer model. The suggested method
involved averaging the frequencies of the first two heptamers of each read. Yet,
we found that in every case, this performed worse than simply counting the
frequencies of the initial heptamer, and thus we report only the latter. The
background frequencies are estimated from positions 18--23 in each read.

Our own method was trained on the 100,000 randomly selected reads from
chromosomes 1--8, considering the initial read position and 20 nucleotides
upstream and downstream.

All data sets were mapped using Bowtie \citep{Langmead2009} using default
parameters against, respectively, the hg19, mm9, rheMac2, and panTro2 genome
assemblies obtained from the UCSC Genome Browser \citep{Karolchik2008}.



\subsection{Kullback-Leibler Divergence}

Plotting the nucleotide frequencies (Figure  \ref{fig:freqs}) we observe an
obvious bias. To quantify the non-uniformity observed in these plots, we use the
symmetrized Kullback-Leibler (KL) divergence \citep{Kullback1951}.

If $f_x$ is the background frequency of a $k$-mer $x$, and $f'_x$ the observed
frequency, the KL divergence is computed as
$$D_k( f, f' ) = \sum_{x} \left( f_x \log_{2}( f_x / f'_x ) + f'_x \log_{2}( f'_x / f_x) \right)$$
where the sum is over all $k$-mers. This can be thought of as a measure
dissimilarity between two probability distributions. If $f_x$ and $f'_x$ for a
$k$-mer $x$ are approximately equal, their log-ratio will be approximately
zero, leading to a small KL divergence (exactly zero, when the distributions are equal).
Conversely, very different $k$-mer frequencies will result in a larger KL
divergence.

When computing the KL divergence, there is a risk of the measure being dominated
by a small number of reads with many duplicates. Yet, given the high coverage of
the exons being tested, if duplicate reads are excluded, it may not capture the
full effect of bias correction. To account for these opposing concerns we adopt
the following method: all reads contained within the exon being tested are
ranked by the number of duplicates. We then exclude reads that are ranked in the
lower half, and count each read ranked in the upper half only once, ignoring
duplicates.

Under the assumption of uniform sampling, the set of reads ranked in the upper
half should not depend on sequence, and we should expect the KL divergence to be
low.  We compute the divergence by reweighting the read counts using the
predicted bias coefficient before ranking the reads, choosing those reads
ranked in the upper half of each exon, ignoring duplicate reads, and then
tallying frequencies of overlapping $k$-mers. The $k$-mer distribution obtained
is then compared to a background distribution obtained by redistributing reads
uniformly at random within their exons.

We repeated the procedure for $k \in \{1, 2, 3, 4, 5, 6\}$. The results of this
analysis are plotted in Figure \ref{fig:kl}, for $k = 1$ and $k = 4$. The
remaining cases are plotted in Supplementary Section 4.

\begin{figure*}
\centerline{\includegraphics[width=0.95\textwidth]{kl.eps}}
\caption{The Kullback-Leibler divergence compares the frequency of $k$-mers
(here, for $k = 1$ and $k = 4$) surrounding the starts of aligned reads to the
frequencies expected under the assumption of uniform sampling from within exons.
A large divergence indicates significant bias. Plotted here is the divergence
from unadjusted read counts as well as after adjusting read counts using each
method.}
\label{fig:kl}
\end{figure*}


\subsection{Poisson Regression}

In this comparison, we measure the uniformity of the data, or more precisely
how well the counts conform to a Poisson process.  The assumption of positional
read counts following a Poisson distribution is known to be a poor fit
\citep{Srivastava2010}, but measuring the improvement in the fit derived from
correcting for bias remains a principled and easily interpreted criterion. This
increase in uniformity is illustrated in Figure \ref{fig:overview}.

We perform maximum-likelihood fitting of two models. In the null model, the
Poisson rate is fixed for each test exon. That is, for position $j$ within exon $i$,
the rate is $ \lambda_{ij} = a_i $ where $a_i$ is the parameter being fit. For
comparison, we then fit a model in which the rate is also proportional to the
predicted bias coefficients: $ \lambda'_{ij} = a_i b_{ij} $.

If the null model has log-likelihood $L$, and the bias-corrected model $L'$, a
simple goodness of fit measure is the improvement in log-likelihood (a statistic
commonly known as McFadden's pseudo-coefficient of determination
\citep{McFadden1974}), defined as,
$R^2 = 1 - L'/L$.

This measure can be interpreted as the improvement in fit over the null model,
with $R^2 = 1$ indicating a perfect fit, occurring when the model being
evaluated achieves a likelihood of 1. Smaller number indicate an increasingly
worse fit, with $R^2 = 0$ representing no improvement over the null model, and
$R^2 = 0.5$, for example, indicating the model has a log-likelihood equal to
half that of the null model (a large improvement, corresponding to, for example,
the likelihood increasing over 100-fold if the initial log-likelihood was -9.6,
which is the mean per-position log-likehood under the null model). This measure
has the added advantage that it can take on values less than 0, indicating that
the model has worse fit than the null model (i.e., when adjusting read counts
according the bias coefficients leads to less uniform read coverage).

We compute $R^2$ for each of the test exons, giving us a sense of the variability
of the effectiveness of each model. The results of this analysis are plotted in
Figure \ref{fig:pois}.  To summarize each model with a single number, we can
examine the median $R^2$ value, as listed in Table \ref{tab:pois}. Our method
shows a highly statistically significant improvement in performance over other
methods in all but one comparison, in which the MART method performs equally.

\begin{figure}
\centerline{\includegraphics[width=0.30\textwidth]{pois-boxplot.eps}}
\caption{For each of the 1000 test exons, we compute McFadden's pseudo-coefficient of
determination $R^2$, equivalent to the improvement in log-likelihood under the
bias corrected model. The statistic is positive, and increases as uniformity is
increased, and negative when uniformity is decreased.  Marked with asterisks are
methods over which the BN approach showed a statistically significant improvement
when applied to the same data, according to a one-sided Wilcoxon signed-rank
test. In each of those marked, we observed p-values less than $10^{-23}$.  Boxes
are plotted to mark the 25\%, 50\%, and 75\% quantiles, with whiskers extending
to 1.5 times the inter-quartile range (i.e., the span between the 25\% and 75\%
quantiles), and dots marking more extreme values.
}
    \label{fig:pois}
\end{figure}


\begin{table}
\processtable{
    The median $R^2$ goodness of fit statistic across test exons.
\label{tab:pois}}{
    \begin{tabular}{lcccc}\toprule
          & BN    & MART  & GLM   & 7mer \\\midrule
Wetterbom & \textbf{0.174} & 0.016 & 0.066 & -0.079 \\
Katze     & \textbf{0.280} & 0.243 & 0.158 & 0.033 \\
Bullard   & \textbf{0.267} & 0.163 & 0.224 & 0.157 \\
Mortazavi & \textbf{0.240} & 0.210 & 0.197 & 0.091 \\
Trapnell  & \textbf{0.289} & \textbf{0.289} & 0.248 & 0.138
    \end{tabular}
}
{
    The $R^2$ statistic measures increased uniformity in read coverage, after
    correcting for bias.  Here the median $R^2$ across the test exons is listed
    for each method and sample. A higher $R^2$ indicates a better fit. The
    highest value in each row is highlighted in bold.
}
\end{table}


\subsection{qRT-PCR Correlation}

We used sequencing data previously published by \citet{Au2010} to evaluate the
effect bias correction has on correlation to measurements made by TaqMan RT-PCR,
made available by the the Microarray Quality Control project \citep{Shi2006}. The
RNA-Seq data shows a pattern of bias similar to that seen in the other samples
sequenced on an Illumina platform (see Supplementary Section 6). This evaluation
does not rely on an assumption that qRT-PCR is necessarily more accurate than
RNA-Seq based quantification, only that qRT-PCR is not biased in the same way as
the RNA-Seq data.

To evaluate the efficacy of each of the bias correction methods considered, we
counted reads overlapping each gene, defining the gene by the union of every
transcript in release 60 of the Ensembl gene annotations. Counts were then
normalized by dividing by the length of these genes. We then removed any genes
with a read count less than 10, or that did not correspond to a unique TaqMan
probe.

Each method was trained in a manner identical to that used in the analysis of
section 3.1 and 3.2, but without restricting the training data to the first
eight chromosomes.  After adjusting read counts according to the predicted
sequence bias, we computed the Pearson correlation coefficient $r$ between log
read counts and log TaqMan expression values, which are averaged across three
replicates. These correlations are listed in Table \ref{tab:pcr}. Our method
shows a statistically significant increase in correlation compared to the other
methods.

\begin{table}
\processtable{
    The Pearson correlation coefficient $r$ between log-adjusted read counts and
    log-adjusted TaqMan values.\label{tab:pcr}}{
    \centerline{
\begin{tabular*}{0.4\textwidth}{lrl}
\toprule
Method & Correlation & \\ \midrule
Unadjusted &  0.6650 & $\ast\ast$ \\
7mer &  0.6680 & $\ast\ast$ \\
GLM & 0.6874 & $\ast\ast$ \\
MART & 0.6998 & $\ast$ \\
BN & \textbf{0.7086}
\end{tabular*}}
}
{
We estimated the statistical significance of the improvement in correlation
using the BN method over the other methods using a simple boostrap procedure. A
bootstrap sample is formed by sampling, with replacement, 648 genes from the
original set of the same size. The correlation is then computed for this set,
using the adjusted count from each method. We repeated this procedure one
million times, and counted the number of times each of the competing methods
achieved a higher correlation than the BN method.  Those marked with a sigle
asterisk achieved a higher correlation fewer than 1000 times, resulting in a
p-value less than $10^{-3}$.  Those marked with two asterisks achieved a higher
correlation in none of the bootstrap samples, indicating a p-value less than
$10^{-6}$.
}
\end{table}

\subsection{Robustness}

Training our model on more reads leads to more accurate estimation of bias, but
an increasingly long training time. For example, in our tests, fitting our model to
100,000 reads from the Mortazavi data, training time was approximately 45
minutes, running on one core of a 3 Ghz Intel Xeon processor. However, limiting
the training to 25,000 reads leads to a model that is nearly as accurate while
requiring less than 4 minutes to train. A full discussion of this trade-off is in
Supplementary Section 6.

The quality of the solution depends also on two other parameters: the standard
deviation at which background sequences are sampled, and the weight applied to
the penalty term of the BIC, yet it is not particularly sensitive to their
values. (The median $R^2$ goodness-of-fit statistic used in section 3.2 varied
by less that 25\% as these parameters were varied over a range of $10^4$. See
Supplementary Section 2.) The same is true of the $p_{\text{max}}$ and
$d_{\text{max}}$ parameters, used restrict the in-degree and edge distance of
the model, respectively, in order to control training time. Our tests show that
these parameters need only be greater than zero for an adequate model to trained
for the Mortazavi data.  In all our evaluation, no special tuning of the
parameters was performed, suggesting it can be used effectively across data sets
without any intervention.

Additionally, experimental and theoretical analysis suggest that the procedure
is very resistant to inclusion of extraneous parameters. In Supplementary
Section 11 we prove an upper bound on the probability of our model predicting
any bias, if the experiment is in fact unbiased, showing that there very little
risk in the applying the method to an unbiased data set. In particular, if more
than 10,000 reads are used in the training procedure, the probability that any
adjustment at all will is made is less than 0.0004.

\section{Discussion}

We have demonstrated that sequence bias can confound, sometimes severely,
quantification in RNA-Seq experiments, and we have introduced an effective
method to account for this bias without the need of existing gene annotations.
The analysis provided demonstrates that our method shows significant improvement
in three aspects: uniformity of read coverage, and consistency of nucleotide
frequencies, and agreement with qRT-PCR.

In our results, estimating initial heptamer frequencies was not seen to be as
effective as the other models, even when data generated using random hexamer
priming was used. A possible explanation is, given the large number of
parameters needed to estimate heptamer frequencies ($4^{7} = 16,383$), these
parameters are estimated with less accuracy than in models requiring fewer
parameters. Yet, we trained the 7mer model on a minimum of 1.9 million reads
(see Supplementary Section 3), a number which theoretical results,
following from work by \citet{Birch1964} and included in Supplementary Section
10 for completeness, suggest should lead to accurate estimates

A perhaps more significant factor is that this method does not capture bias
outside of the initial heptamer, though many data sets clearly are affected by
bias in other positions.  Thus to improve the performance, it seems necessary to
increase the size of the $k$-mers being considered. However, exponentially more reads
would be required for an accurate estimate since the accuracy of the model, as
quantified by its KL divergence from the true distribution, is $(r-1)/2n$, where
$r=4^k$ is the number of parameters that must be estimated (See Supplementary
Section 10).

Our method generalizes this approach, attempting to overcome this problem by
using an estimation of sequence probability that requires fewer parameters
and can account for bias outside of the initial heptamer. In all our tests, this
approach was at least as effective as those of \citet{Li2010}, despite not
requiring gene annotations or manual selection of training examples.

We have not performed any direct comparison to the method described by
\citet{Roberts2011} and implemented in Cufflinks \citep{Trapnell2010}. Though
this method is superficially similar to our own, a proper comparison is
difficult, as the software cannot be applied independently of estimating
transcript abundance in FPKM (fragments per exonic kilobase, per million mapped
reads) using Cufflinks. Fairness would dictate that competing methods be
substituted in FPKM estimation, or that a separate interface be written to the
Cufflinks bias correction method---both comparisons requiring significant effort.

Though the Cufflinks method and our own both use graphical models to estimate
sequence probabilities, we make no restriction on the graph other than
acyclicity. We go to considerable effort to efficiently approximate the optimal
structure for each data set rather than using a fixed structure, as in
Cufflinks. A ``one size fits all'' approach likely works quite well in many
cases, yet the observed specificity of the bias to protocol and platform argues
against it. For example, the structures learned by our method (Figure
\ref{fig:models}) are considerably different between those sequenced on an
Illumina platform versus an ABI platform, and even vary within platform.

During the review of this manuscript, another method addressing sequence bias in
RNA-Seq was published by \citet{Zheng2011}. Rather than fitting a model of the
specific base-level sequence bias surrounding read start, they propose making
adjustments according to summary statistics at the gene level. Such an approach
is disadvantaged in its inability to model the very specific pattern of sequence
bias we have observed. Yet, such an approach is efficient, and though we have
not yet evaluated it, claimed to be effective.

Because we do not require annotations, ChIP-Seq, and other high-throughput
sequencing experiments, may also benefit from our model. In a preliminary
investigation, we found the sequence bias in one ChIP-Seq experiment
\cite{Cao2010} was less than that observed in any of the RNA-Seq data we
evaluated; however, our method is still able to effectively correct for the bias
that was observed (see Supplementary Section 7).  Protocol differences, as we
have seen, can result in significant differences in observed nucleotide
frequencies, so we can not safely assert that bias in ChIP-Seq data is always
low.  Given the weak assumptions made by our model, our estimation of bias could
easily be incorporated into ChIP-Seq peak-calling algorithms, and potentially
improve accuracy.

To determine the extent to which PCR amplification is responsible for the
observed bias, we evaluated data from the FRT-Seq method proposed by
\citet{Mamanova2010}. FRT-Seq avoids the PCR amplification step during library
preparation with reverse transcription occuring on the flowcell surface. We
observed that this data is not free from sequence bias, yet unlike other data
generated on the Illumina platform, it appears to be effected only by relatively
few positions adjacent to the read start (see Supplementary Section 8). Other
protocol improvements might further reduce sequence bias. Notably, promising work
by \citet{Jayaprakash2011} proposes a pooled adapter strategy to deal with this
issue in small RNA sequencing experiments.

RNA-Seq is most often used to compare levels of expression, and so a natural
concern is the consistency of the bias between samples. In the data we examined,
the bias appears to be largely, but not entirely consistent (see Supplementary
Section 9).  Similarly, in Figure \ref{fig:freqs}, the three data sets sequenced
on the Illumina platform display similar patterns of nonuniformity, yet differ
in magnitude, suggesting that batch effects in RNA-Seq remain a legitimate
concern that should not be dismissed without evaluation.

In summary, we have demonstrated a relatively simple graphical model that
effectively corrects for sequence bias pervasive in RNA-Seq, and to a lesser
extent, ChIP-Seq experiments. In our tests, this model performs at least as
well, and often better than existing methods, and involves fewer requirements or
assumptions. Our model leads to more accurate quantification, and would likely
provide a positive benefit when incorporated into downstream analysis.


\paragraph{Funding\textcolon} This project has been funded in whole or in part
with Federal funds from the National Institute of Allergy and Infectious
Diseases, National Institutes of Health, Department of Health and Human Services,
under Contract No.  HHSN272200800060C and by the Public Health Service grant
P51RR000166 from the National Institutes of Health.

%\vspace{-8em}

\bibliographystyle{natbib}
\bibliography{seqbias-paper}

\end{document}



