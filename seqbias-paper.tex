
\documentclass{bioinfo}
\copyrightyear{2011}
\pubyear{2011}

\usepackage{comment}


\begin{document}
\firstpage{1}

\title{Correcting for bias in high-throughput sequencing data}
\author[Jones \textit{et~al}]
{Daniel C. Jones\,$^{1,}$
\footnote{to whom correspondence should be addressed}\hspace{0.5em}
Walter L. Ruzzo\,$^{1,2,3}$
Xinxia Peng\,$^{4}$
Michael G. Katze$^{4}$
}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\address{
$^{1}$Department of Computer Science and Engineering, University of
Washington, Seattle, WA 98195-2350, USA\\
$^{2}$Department of Genome Sciences, University of Washington, Seattle, Wa
98195-5065, USA\\
$^{3}$Fred Hutchinson Cancer Research Center, Seattle, WA 98109, USA\\
$^{4}$Department of Microbiology, University of Washington, Seattle, WA
98195-7242, USA}

\editor{Associate Editor: XXXXXXX}

\maketitle

\begin{abstract}

\section{Motivation:}
Quantification of sequence abundance in RNA-Seq and ChiP-Seq experiments is
often conflated by protocol-specific sequence bias, influenced by PCR
amplification, or differing primer affinities and mixtures, for example.  The
result is decreased accuracy in many applications, such as de novo gene
annotation and isoform quantification in RNA-Seq, and peak calling in
ChIP-Seq.


\section{Results:}
We present a method to measure and correct for these influences using a simple
graphical model. Our model does not rely on existing gene annotations, making it
applicable to any high throughput sequencing data. We evaluate our method on
several data sets, and by multiple criteria, demonstrating that it effectively
decreases bias and increases uniformity.


\section{Availability:}
The method is implemented in the \texttt{seqbias} R/Bioconductor package,
available freely under the LGPL license from
\href{http://bioconductor.org}{http://bioconductor.org}.


\section{Contact:}
\href{dcjones@cs.washington.edu}{dcjones@cs.washington.edu}

\end{abstract}


\section{Introduction}

In the last few years, RNA-Seq has emerged as a promising alternative to
microarrays in quantifying RNA abundance. But, as microarray technology has
brought with it technical challenges ranging from developing robust
normalization to accounting for cross-hybridization, RNA-Seq presents a new set
of problems.

A significant example is the effect of transcript length on quantification.  As
noted by \citet{Oshlack2009} transcript length biases discovery of
differential expression, since more reads are aligned to longer transcripts,
granting higher statistical significance in tests of differential expression.
Recently, some methods have been developed attempting to account for this effect
in gene set enrichment tests \citep{Young2010,Gao2011}, leading to more accurate
downstream analysis.

The underlying nucleotide sequence also influences quantification in a manner
that is complex and protocol-specific. In an ideal experiment, the number of
reads mapping to a particular position in the genome is a function of RNA
abundance and should not be additionally dependent on the sequence at that
position. As noted first by \citet{Dohm2008}, this is not the case.  As
illustration, Figure \ref{fig:freqs} plots this nonuniformity in nucleotide
frequencies on five data sets (see, Table \ref{tab:datasets}), each using a
different protocol.


These biases may adversely effect transcript discovery, as low level noise may
be over-reported in some regions, and in others, active transcription may be
under-reported. They render untrustworthy comparisons of relative abundance
between genes or isoforms, and any test of differential expression hangs on the
assumption that these biases are identical between replicates.  Additionally, in
many tests of differential expression---similar to the effects of transcript
length---a higher read count will result in higher statistical confidence. It
follows that the sensitivity of such a test will also be biased by sequence,
affecting downstream analysis such as gene ontology enrichment tests.

This bias, though observed primarily in the $5'$ end of a read, is not resolved by
trimming the reads prior to mapping \citep{Hansen2010} (see also, Supplementary
Section 1), suggesting it is not a result of erroneous base calling, and that a
more sophisticated means of correction is needed.

\begin{figure*}
\centerline{\includegraphics[width=0.85\textwidth]{freqs.eps}}
\caption{Nucleotide frequencies are plotted relative to the start (labeled
position 0) of each mapped read, respecting strand, and grouped by platform
(Illumina or ABI SOLiD).  The data sets plotted here are those used
for evaluation, listed in Table \ref{tab:datasets}.
The sequence is taken from the genomic context surrounding the read, so that -40
to -1, for example, fall outside the read sequence itself. The symmetrized
Kullback-Leibler divergence is used to summarize the difference in nucleotide
frequency compared to a fixed estimate of background nucleotide frequencies made
by sampling many positions near mapped reads.  Under the assumption that reads
are sampled uniformly from transcripts, each of the plots should be essentially
flat}
\label{fig:freqs}
\end{figure*}

\citet{Li2010} propose two models. The first is a Poisson linear
model, in which read counts across a transcript follow an inhomogeneous Poisson
process. The read count at position $i$ within the transcript is Poisson
distributed with parameter $\lambda_i$, where, $\log(\lambda_i)$ is the sum of
weights determined by the nucleotide at each position surrounding the read
start, in addition to a term capturing the abundance of the transcript.

The second model is based on multiple additive regression trees, or MART
\citep{Friedman2003}.  In their tests, the MART model shows a moderate
improvement over the Poisson linear model. Both models are fit to a number of
abundant test genes, requiring existing gene annotations for the reference
genome. 

Another model, proposed by \citet{Hansen2010}, directly estimates the
distribution of initial heptamers within reads, then estimates a presumed
background heptamer distribution, sampled from the ends of reads. The read count
at a given position is then adjusted by the ratio of the foreground and
background heptamer probabilities. Specifying two distributions over heptamers
(i.e. foreground and background distributions) requires \mbox{$2(4^7-1) =
32,766$}
parameters, so while no gene annotations are needed to train such a model, a
significant number of reads are required.

Lastly, \citet{Roberts2011} have very recently published a description of
another approach, in which sequence probabilities are modeled by variable-order
Markov chains. The structure of these Markov chains are hard-coded, chosen in
advance using a hill-climbing algorithm on a representative data set. This method
is implemented in the latest version of Cufflinks \citep{Trapnell2010}, and
tightly incorporated into its estimation of transcript abundance, requiring
either predicted or existing gene annotations.

Here we propose a new approach, using Bayesian networks to model sequence
probabilities. Unlike the methods of Roberts or Li, our model requires no gene
annotations, nor even the assumption that the short reads are derived from RNA.
In this sense, we build on the work done by \citet{Hansen2010}, generalizing
their approach in a way we find to be more robust and effective at correcting
for bias in a variety of protocols. Due to the weak assumptions required by our
model, it is applicable and potentially useful in any setting in which short
reads are aligned to a reference sequence.

\section{Methods}

\subsection{Principle}

We begin with a natural model of an RNA-Seq experiment (and one that is often
assumed, whether implicitly or otherwise). The number of reads $x_i$ aligned to
genomic position $i$ is an unbiased estimate of RNA abundance. Furthermore, we
assume reads may be treated as independent and identically distributed samples.
That is, if $N$ reads are generated, and $m_i$ is the event that a generated
read maps to position $i$.
$$ E[x_i] = N \Pr[m_i] $$

The experiment may be considered unbiased  with regards to sequence if, having
observed the nucleotide sequence $s_i$ surrounding position $i$,
$$ E[ x_i | s_i ] = N \Pr[ m_i | s_i ] = N \Pr[ m_i ] = E[ x_i ] $$

From Bayes' rule,
$$ \Pr[ m_i | s_i ] = \frac{ \Pr[ s_i | m_i ] \Pr[ m_i ] }{ \Pr[ s_i ] } $$

This suggests a natural scheme in which observations may be reweighted to
correct for bias.  First, define the \emph{sequence bias} $b_i$ at position $i$
as $ b_i = \Pr[ s_i ] /  \Pr[ s_i | m_i ]  $.

Now, if we reweight the read count $x_i$ at position $i$ by $b_i$, we
have,
\begin{align*}
E[ b_i x_i | s_i ] &= b_i E[ x_i | s_i ] \\
&= N b_i \Pr[ m_i | s_i ] \\
&= N \frac{ \Pr[ m_i | s_i ] \Pr[ s_i ] }{ \Pr[ s_i | m_i ] } \\
&= N \Pr[ m_i ] \\
&= E[ x_i ]
\end{align*}
Thus the reweighted read counts are made unbiased.

To estimate the bias $b_i$, we must make estimates of the background sequence
probability $\Pr[s_i]$ and the foreground sequence probability $\Pr[ s_i | m_i
]$, the latter being the probability of the sequence given a read being sampled
from its position. Estimating bias is therefore a problem of finding a model of
sequence probability that is sufficiently complex to capture the common features
of the training data yet avoids overfitting.

Towards that end, we propose training a Bayesian network classifier trained on
examples of foreground and background sequences. Though our goal is not
classification, by using the machinery of classification we can avoid a model
that is over-parametrized. Parameters that are not informative in
discriminating between foreground and background are not included in the model.
The Bayesian network can then be used to evaluate sequence probability, and thus
bias, at any genomic position. Figure \ref{fig:overview} gives a brief overview
of the proposed model.

\begin{figure}
\centerline{\includegraphics[width=0.4\textwidth]{overview.eps}}
\caption{An overview of the approach taken: (a) foreground sequences are sampled
from the regions surrounding the starts of mapped reads, (b) background
sequences are sampled by randomly offsetting foreground positions, (c) a
Bayesian network is trained to discriminate between the set of
sampled foreground and background sequences, (d) and the model is evaluated
at each position within a locus, predicting bias. The predicted bias can then be
used to adjust read counts, as in (e). In (d) and (e) we show the results of
this method applied to the $3'$ UTR of Apoa2, using data from
\citet{Mortazavi2008}.}
\label{fig:overview}
\end{figure}

However, we have so for ignored one complication: the RNA abundance that we wish
to estimate is not itself independent of the nucleotide sequence. Notably,
exonic DNA tends to be more GC-rich than intergenic DNA. If background sequences
are sampled uniformly from the genome we run the risk of incorrectly adjusting
for biological sequence bias, rather than technical sequence bias.  To avoid
this, we propose using paired training data. Each foreground training
sequence is paired with a background sequence taken from a nearby position
that is likely to have similar abundance and general nucleotide composition.
Alternatively, we could pair foreground samples with background samples from
within the same transcript, but we prefer to avoid dependence on existing
gene annotations as such a model would be limited to RNA-Seq analysis in which
trustworthy and reasonably complete gene annotations are available.

The methods proposed by \citet{Hansen2010} and \cite{Roberts2011} also treat bias
correction as a problem of estimating foreground and background sequence
probabilities. They differ primarily in how these sequence probabilities are
estimated. \citet{Li2010} estimate reweighting coefficients ($b_i$, in our
notation) directly, given training data consisting of long, highly expressed
transcripts.



\subsection{Estimation}

To estimate sequencing bias, we train a Bayesian network in which each node
represents a position in the sequence, relative to the read start, and edges
encode dependency between positions.  Bayesian networks have been applied to
recognize motifs in nucleotide sequences in the past, in particular in modeling
splice sites \citep{Cai2000, Chen2005} and transcription factor binding sites
\citep{Ben-Gal2005, Grau2006, Pudimat2005}. 

In our model, we do not rely on constraining the set of networks (e.g., to
trees), and instead approximate the NP-Hard problem of determining the optimal
network structure using a fast hill-climbing algorithm. Furthermore, we
train our model \emph{discriminatively}; only parameters that are deemed
informative in discriminating between foreground and background sequences are
included in the model. We thus seek to train a model that reduces bias, without
including uninformative parameters that would only increase variance.


\subsubsection{Sampling}

The model is trained on $n$ sequences, one half labeled as foreground, the other
background, sampled from the reference genome. To obtain the foreground
sequences, we take sequences surrounding (extending 20 nt to either side, by
default) the starts positions of $n/2$ aligned reads. We wish to avoid
overfitting to any short locus with a large abundance of reads, yet we must also
capture the bias introduced by, for example, PCR amplification. We therefore
choose the $n/2$ unique reads with the highest number of duplicates, yet count
each read only once, ignoring duplicates

To obtain background training sequences, we randomly offset the positions from
which the foreground sequences were sampled.  The offset is drawn from a
zero-mean Gaussian (with $\sigma^2 = 10$, by default), and rounded to the
nearest integer, away from zero.  By using such a scheme, we attempt to mitigate
the effects of biological sequence bias, sampling positions that are more likely
to be biologically similar.

This procedure produces a training set of $n$ sequences with accompanying labels
$T = \{ (s_1, x_1), (s_2, x_2), \dots, (s_n, x_n) \}$. The label $x_i$ is
binary, indicating classification as background ($x_i = 0$) or foreground ($x_i
= 1$).

\subsubsection{Training}


To determine the structure and parameters of the Bayesian network, we use a
hill-climbing approach similar to the algorithm described by \citet{Grossman2004}.
The network structure is determined by greedily optimizing the conditional
log-likelihood:
\begin{align*}
\ell &= \sum_{i=1}^{n} \log \Pr( x_i | s_i ) 
=
\sum_{i=1}^{n} \log \frac{ \Pr(s_i | x_i) \Pr( x_i ) }{
\sum_{x \in \{0,1\}} \Pr( s_i | x ) \Pr(x) } \\
\end{align*}
where $\Pr(x)$ is flat (i.e.  $\Pr( x = 0 ) = \Pr( x = 1 ) =
0.5$) if we sample foreground and background positions equally.

As we will be estimating parameters and evaluating the likelihood on the same
set of samples, simply maximizing the likelihood would severely overfit the
training set. We thus penalize model complexity heuristically using the Bayesian
Information Criterion \citep{Schwarz1978a}. Where $m$ is the number of
parameters needed to specify the model, we maximize, 
$$ \ell' = 2 \ell - m \log n $$

Some benefit would be obtained from a more highly tuned complexity penalty.
However, since the model is trained greedily, additional parameters will be
decreasingly informative, and increasingly similar between foreground and
background. Adding more parameters will have little effect.  Only when $m$ is
allowed to grow exponentially does the prediction become polluted by small
deviations between thousands of uninformative parameters.

At each step of the optimization procedure, every possible edge or position
addition, removal, or edge reversal that produces a valid network (i.e., does not
introduce a cycle) is evaluated, and the alteration that increases the score
$\ell'$ the most is kept.  This process is repeated until a local maximum is
found, in which no single alteration to the network will increase the score.

Given the network structure, the parameters are estimated directly from the
observed nucleotide frequencies in the training data.

The run time of the training procedure is further reduced in practice by imposing the
following two restrictions on the structure of the network,
\begin{enumerate}
\item The in-degree (i.e. number of parents) of any node must be less than some
number $p_{\text{max}}$.
\item $|j - i| \le d_{\text{max}}$ for all edges $(i,j)$ and some number $d_{\text{max}}$.
\end{enumerate}
The second of these encodes the assumption that distant nucleotides are
effectively independent. We choose $p_{\text{max}} = 4$ and $d_{\text{max}} =
10$, as reasonable default values.

Along with $p_{\text{max}}$ and $d_{\text{max}}$, the time required to train the
model is controlled by the number of reads used (i.e., $n$) and the number of
positions surrounding the read start to consider. Increasing any of these
parameters results in a solution that is no worse, and potentially better, at
the expense of increased training time.

The quality of the solution depends also on two other parameters: the standard
deviation at which background sequences are sampled, and the weight applied to
the penalty term of the BIC, yet it is not particularly sensitive to their values
(see, Supplementary Section 2). In all our evaluation, no special tuning of the
parameters was performed, suggesting it can be used effectively across data sets
without any intervention.

Figure \ref{fig:models} shows examples of the structure learned when this
procedure is applied to several data sets, using 100,000 reads from each.


\section{Results}

\begin{figure*}
\centerline{\includegraphics[width=\textwidth]{models.eps}}
\caption{
The network structures learned on each of the data sets are displayed. Positions
are relative to the read start, which is labeled 0. Hollow circles indicate
positions that were not included in the model, being deemed uninformative, given
the other positions and edges. The number of parameters needed to
specify each model is listed in parenthesis below. Applied to data with less
bias, a sparser model is trained, as evinced by the Wetterbom data set.
}
\label{fig:models}
\end{figure*}


Because we cannot observe directly the underlying RNA-abundance, our evaluation
strategy relies on testing three assumptions we make of an ideal, unbiased experiment.
\begin{enumerate}
\item Positional nucleotide frequencies (as in Figure \ref{fig:freqs}), measured
from reads within exons, should not differ greatly from frequencies measured by
sampling uniformly within the same exons.
\item Read counts across a single exon should follow, approximately, a Poisson
process.
\item Adjusting for bias should increase the agreement between RNA-Seq and
qRT-PCR.
\end{enumerate}

We applied our procedure (labeled ``BN'') as well as those of
\citet{Li2010} (``GLM'' and ``MART'') and \citet{Hansen2010} (``7mer''), which
are implemented in the R packages \texttt{mseq} and \texttt{Genominator},
respectively. The procedures were applied to four publicly available data
sets \citep{Bullard2010, Mortazavi2008, Trapnell2010, Wetterbom2010}, as well as
an unpublished data set of our own (Table \ref{tab:datasets}).

\begin{table}
\processtable{Data sets on which the methods are evaluated.\label{tab:datasets}}
{
\begin{tabular}{lllcc}\toprule
 & & & Read & Num. of \\
Experiment & Species & Platform & length & reads \\\midrule
\citet{Wetterbom2010} & Chimpanzee & ABI & 33 & $3.9 \times 10^7$ \\
Katze (unpublished) & Macaque & ABI & 50  & $4.8 \times 10^7$ \\
\citet{Bullard2010} & Human & Illumina & 35 & $1.2 \times 10^7$ \\
\citet{Mortazavi2008} & Mouse & Illumina & 33 & $3.9 \times 10^6$ \\
\citet{Trapnell2010} & Mouse & Illumina & 75 & $6.6 \times 10^7$ \\\botrule
\end{tabular}
}{}
\end{table}


Testing was performed by cross-validation. Each method was trained on data taken
from chromosomes 1--8 of the genome from which the reads were mapped (including
chromosomes 2a and 2b of the Chimpanzee genome). For evaluation, we drew a set
of long, highly expressed exons from the remaining chromosomes. In particular, for
each reference sequence, beginning with the set of exons annotated by Ensembl
release 60 \cite{Hubbard2009}, we removed any exons with known alternate splice
sites, then chose the top 1000 exons by read count, restricting ourselves those
at least 100 nt long.

The differences in the methods being tested necessitated training procedures
unique to each.

\citet{Li2010} recommends that their MART and GLM models be trained using the
100 most abundant genes. We used 1000 exons from chromosomes 1--8, otherwise
chosen in a manner identical to that which was used to select the test exons.
Both the GLM and MART models were trained using the default parameters.

\citet{Hansen2010} recommends using all of the reads to estimate heptamer
frequencies used by their model. The training procedure works by simple tallying
of frequencies. The implementation of this model in the Genominator package
uses a great deal of memory, and we were unable to train with the volume of
data we wished, so we reimplemented the model and trained it on all of the reads
aligned to chromosomes 1--8.

We evaluated several variations of the heptamer model. The suggested method
involved averaging the frequencies of the first two heptamers of each read. Yet,
we found that in every case, this performed worse than simply counting the
frequencies of the initial heptamer, and thus we report only the latter. The
background frequencies are estimated from positions 18--23 in each read.

Our own method was trained on the 100,000 reads from chromosomes 1--8, with the
highest number of duplicate reads.

All data sets were mapped using Bowtie \citep{Langmead2009} against,
respectively, the hg19, mm9, and panTro2 genome assemblies obtained from the
UCSC Genome Browser \citep{Karolchik2008}.



\subsection{Kullback-Leibler Divergence}



Plotting the nucleotide frequencies (Figure  \ref{fig:freqs}) we observe an
obvious bias. To quantify the non-uniformity observed in these plots, we use the
symmetrized Kullback-Leibler (KL) divergence \citep{Kullback1951}.

If $f_x$ is the background frequency of a $k$-mer $x$, and $f'_x$ the observed
frequency, the KL divergence is computed as
$$D_k( f, f' ) = \sum_{x} \left( f_x \log( f_x / f'_x ) + f'_x \log( f'_x / f_x) \right)$$
where the sum is over all $k$-mers.


To avoid the risk of a small number of reads with many duplicates dominating the
measure, we do not count duplicate reads when computing the KL divergence. Since we
explicitly chose exons with high coverage, not counting duplicates may not
fully capture the effects of the bias correction methods. Thus, for each exon,
we compute the number of duplicates of each read and count only reads in the
upper half. Under the assumption of uniform sampling, the set of reads falling
in the upper half should not depend on sequence, and we should expect the KL
divergence to be low.

We compute the divergence by reweighting the read counts using the predicted
bias coefficient, choosing the upper half in each exon, discarding duplicate
reads, and then tallying frequencies of overlapping $k$-mers. The $k$-mer
distribution obtained is then compared to a background distribution obtained by
redistributing reads uniformly at random within their exons.

We repeated the procedure for $k \in \{1, 2, 3, 4, 5, 6\}$. The results of this
analysis are plotted in Figure \ref{fig:kl}, for $k = 1$ and $k = 4$. The
remaining cases are plotted in Supplementary Section 3.

\begin{figure*}
\centerline{\includegraphics[width=0.95\textwidth]{kl.eps}}
\caption{The Kullback-Leibler divergence compares the frequency of $k$-mers
(here, for $k = 1$ and $k = 4$) surrounding the starts of aligned reads to the
frequencies expected under the assumption of uniform sampling from within exons.
A large divergence indicates significant bias. Plotted here is the divergence
from unadjusted read counts as well as after adjusting read counts using each
method.}
\label{fig:kl}
\end{figure*}


\subsection{Poisson Regression}

In this comparison, we measure how well the counts conform to a Poisson process.
The assumption of positional read counts following a Poisson distribution is
known to be a poor fit \citep{Srivastava2010}, but measuring the improvement in
the fit derived from correcting for bias remains a principled and easily
interpreted criterion. This increase in uniformity is illustrated in Figure
\ref{fig:overview}.

We perform maximum-likelihood fitting of two models. In the null model, the
Poisson rate is fixed for each exon. That is, for position $j$ within exon $i$,
the rate is $ \lambda_{ij} = a_i $ where $a_i$ is the parameter being fit. For
comparison, we then fit a model in which the rate is also proportional to the
predicted bias coefficients: $ \lambda'_{ij} = a_i b_{ij} $.

If the null model has log-likelihood $L$, and the bias-corrected model $L'$, a
simple goodness of fit measure is the improvement in log-likelihood (a statistic
commonly known as McFadden's pseudo-coefficient of determination
\citep{McFadden1974}), defined as,
$R^2 = 1 - L'/L$.

This measure can be interpreted as the improvement in fit over the null model,
with $R^2 = 1$ indicating a prefect fit, and smaller numbers, an increasingly
worse fit. A more dubious, yet intuitive interpretation of $R^2$ is the
proportion of variability explained by the model.

We compute $R^2$ for each of the test exons, giving us a sense of the variability
of the effectiveness of each model. The results of this analysis are plotted in
Figure \ref{fig:pois}.  To summarize each model with a single number, we can
examine the median $R^2$ value, as listed in Table \ref{tab:pois}.

\begin{figure}
\centerline{\includegraphics[width=0.30\textwidth]{pois-boxplot.eps}}
\caption{For each of the 1000 test exons, we compute a pseudo-coefficients of
determination $R^2$, equivalent to the improvement in log-likelihood under the
bias corrected model. The statistic is positive, and increases as uniformity is
increased, and negative when uniformity is decreased.  Marked with stars are
methods over which our approach showed a statistically significant improvement
when applied to the same data, according to a one-sided Wilcoxon signed-rank
test. In each of those marked, we observed p-values less than $10^{-23}$.  Boxes
are plotted to mark the 25\%, 50\%, and 75\% quantiles, with whiskers extending
to 1.5 times the inter-quartile range (i.e., the span between the 25\% and 75\%
quantiles), and dots marking more extreme values.
}


    \label{fig:pois}
\end{figure}


\begin{table}
\processtable{
    The median $R^2$ goodness of fit statistic across test exons.
\label{tab:pois}}{
    \begin{tabular}{lcccc}\toprule
          & BN    & MART  & GLM   & 7mer \\\midrule
Wetterbom & \textbf{0.174} & 0.016 & 0.066 & -0.079 \\
Katze     & \textbf{0.280} & 0.243 & 0.158 & 0.033 \\
Bullard   & \textbf{0.267} & 0.163 & 0.224 & 0.157 \\
Mortazavi & \textbf{0.240} & 0.210 & 0.197 & 0.091 \\
Trapnell  & \textbf{0.289} & \textbf{0.289} & 0.248 & 0.138
    \end{tabular}
}
{
    The $R^2$ statistic measures increased uniformity in read coverage, after
    correcting for bias.  Here the median $R^2$ across the test exons is listed
    for each method and sample. A higher $R^2$ indicates a better fit.
}
\end{table}


\subsection{qRT-PCR Correlation}

We used sequencing data previously published by \citet{Au2010} to evaluate the
effect bias correction has on correlation to measurements made by TaqMan RT-PCR,
made available by the the Microarray Quality Control project \citep{Shi2006}. The
RNA-Seq data shows a pattern of bias similar to that seen in the other samples
sequenced on an Illumina platform.
%% TODO: refer to supplement for figure of bias

To evaluate the efficacy of each of the bias correction methods considered, we
counted reads overlapping each gene, defining the gene by the union of every
transcript in release 62 of the Ensembl gene annotations, excluding UTRs, and
100 additional bases from both ends of the gene. Counts were then normalized
(divided) by the length of these genes. We then removed any genes with a read
count less than 10, or that did not correspond to a unique TaqMan probe, leaving
592 genes.

%% TODO: something goes here
After adjusting read counts according to the predicted sequence bias, we
computed the Pearson correlation between log read counts and log TaqMan
expression values, which are averaged across three replicates.


%\begin{figure}
%\centerline{\includegraphics[width=0.4\textwidth]{pcrcor.eps}}
%\caption{
    %Pearson correlation is computed betweer TaqMan values and (bias-corrected)
    %read counts, on a logarithmic scale.
    %%%TODO
    %Error bars show a 95\% confidence interval, estimated using the Fisher
    %transformation, as computed by the R function \texttt{cor.test}.
%}
%\label{fig:pcrcor}
%\end{figure}


\begin{table}

\processtable{
    Pearson correlation between log-adjusted read counts and log-adjusted TaqMan
    values.\label{tab:pcr}}{
    \centerline{
\begin{tabular*}{0.4\textwidth}{lrl}
\toprule
Method & Correlation & \\ \midrule
Unadjusted &  0.6665 & $\ast\ast$ \\
7mer &  0.6677 & $\ast\ast$ \\
GLM & 0.6854 & $\ast\ast$ \\
MART & 0.6921 & $\ast$ \\
BN & \textbf{0.7016}
\end{tabular*}}

}
{
We estimated the statistical significance of the improvement in correlation
using the BN method over the other methods using a simple boostrap procedure. A
bootstrap sample is formed by sampling, with replacement, 592 genes from the
original set of the same size. The correlation is then computed for this set,
using the adjusted count from each method. We repeated this procedure one
million times, and counted the number of times each of the competing method achieves a
higher correlation than the BN method.  Those marked with a sigle asterisk
achieved a higher correlation fewer than 1000 times, resulting in a p-value of
less than $10^{-3}$ Those marked with two asterisks achieved a higher
correlation in none of the bootstrap samples, indicating a p-value less than
$10^{-6}$.
}
\end{table}



\section{Discussion}

We have demonstrated that sequence bias can confound, sometimes severely,
quantification in RNA-Seq experiments, and we have introduced an effective
method to account for this bias without the need of existing gene annotations.
The analysis provided demonstrates that our method shows significant improvement
in two aspects: uniformity of read coverage, and consistency of nucleotide
frequencies. These results strongly suggest an increase in accuracy, and we show
in Supplementary Section 4, that this conclusion is further justified by
comparison to qRT-PCR.

In our results, estimating initial heptamer frequencies was not seen to be as
effective as the other models, even when data generated using random hexamer
priming was used. Given the large number of parameters needed to estimate
heptamer frequencies, a likely explanation is that the model is overfit to the
training set.  We trained the 7mer model on a minimum of 600,000 reads (with the
Bullard data set), and up to 23 million reads (with the Wetterbom data set), yet
it seems more reads, or a greater diversity of reads are needed to produce an
accurate or useful estimate.

Our method generalizes this approach, attempting to overcome this problem by
using an estimation of sequence probability that is more robust to overfitting
and can account for bias beyond the initial heptamer. In all our tests, this
approach was at least as effective as those of \citet{Li2010}, despite not
requiring gene annotations or manual selection of training examples.

We have not performed any direct comparison to the method described by
\citet{Roberts2011} and implemented in Cufflinks \citep{Trapnell2010}. Though
this method is superficially similar to our own, a proper comparison is
difficult, as the software cannot be applied independently of estimating
transcript abundance in FPKM (fragments per exonic kilobase, per million mapped
reads) using Cufflinks. Fairness would dictate that competing methods be
substituted in FPKM estimation, or that a separate interface be written to the
Cufflinks bias correction method---both comparisons requiring significant effort.

Though the Cufflinks method and our own both use graphical models to estimate
sequence probabilities, we make no restriction on the graph other than
acyclicity. We go to considerable effort to efficiently approximate the optimal
structure for each data set rather than using a fixed structure, as in
Cufflinks. A ``one size fits all'' approach likely works quite well in many
cases, yet the observed specificity of the bias to protocol and platform argues
against it. For example, the structures learned by our method (Figure
\ref{fig:models}) are considerably different between those sequenced on an
Illumina platform versus an ABI platform.

Training our model on more reads leads to more accurate estimation of bias, but
an increasingly long training time. For example, in our tests, fitting our model to
100,000 reads from the Mortazavi data, training time was approximately 45
minutes, running on one core of a 3 Ghz Intel Xeon processor. However, limiting
the training to 25,000 reads leads to a model that is nearly as accurate while
requiring less than 4 minutes to train. A full discussion of this trade-off is in
Supplementary Section 5.

Because we do not require annotations, ChIP-Seq, and other high-throughput
sequencing experiments, may also benefit from our model. In a preliminary
investigation, we found the sequence bias in one ChIP-Seq experiment
\cite{Cao2010} was less than that observed in any of the RNA-Seq data we
evaluated; however, our method is still able to effectively correct for the bias
that was observed (see, Supplementary Section 6).  Protocol differences, as we
have seen, can result in significant differences in observed nucleotide
frequencies, so we may not safely assert that bias in ChIP-Seq data is always
low.  Given the weak assumptions made by our model, our estimation of bias could
easily be incorporated into ChIP-Seq peak-calling algorithms, and potentially
improve accuracy.

To determine the extent to which PCR amplification is responsible for the
observed bias, we evaluated data from the FRT-Seq method proposed by
\citet{Mamanova2010}.  FRT-Seq avoids the PCR amplification step during library
preparation with reverse transcription occuring on the flowcell surface. We
observed that this data is not free from sequence bias, yet unlike other data
generated on the Illumina platform, it appears to be effected only by relatively
few positions adjacent to the read start (see, Supplementary Section 7).

RNA-Seq is most often used to compare levels of expression, and so a natural
concern is the consistency of the bias between samples. In the data we examined,
the bias appears to be largely, but not entirely consistent (see, Supplementary
Section 8).  Similarly, in Figure \ref{fig:freqs}, the three data sets sequenced
on the Illumina platform display similar patterns of nonuniformity, yet differ
in magnitude, suggesting that batch effects in RNA-Seq remain a legitimate
concern that should not be dismissed without evaluation.

In summary, we have demonstrated a relatively simple graphical model that
effectively corrects for sequence bias pervasive in RNA-Seq, and to a lesser
extent, ChIP-Seq experiments. In our tests, this model performs at least as
well, and often better than existing methods, and involves fewer requirements or
assumptions. Our model leads to more accurate quantification, and would likely
provide a positive benefit when incorporated into downstream analysis.


\paragraph{Funding\textcolon} This project has been funded in whole or in part
with Federal funds from the National Institute of Allergy and Infectious
Diseases, National Institutes of Health, Department of Health and Human Services,
under Contract No.  HHSN272200800060C and by the Public Health Service grant
P51RR000166 from the National Institutes of Health.



\bibliographystyle{natbib}
\bibliography{seqbias-paper}

\end{document}



