
\documentclass{article}
\usepackage{amsmath}
%\usepackage{dcj}

\begin{document}


We appreciate the time and effort extended by the anonymous reviewers and the
editorial staff to evaluate the original version of this manuscript, which was
submitted on July 8. Upon review, the second of the two reviewers raised some
concerns, compelling its rejection. However, we believe that the bulk of these
concerns are either incongruous with the content of the manuscript or
addressable with very little effort, and therefore do not warrant its rejection.
Here we have provided a detailed response to these comments, and have included
additional material in an effort to clarify these points.

The second reviewer raised three major concerns. The first concern raised is
that the method we present is unjustifiably complex, involving a ``plethora of
tuning parameters'', and that the much simpler method of Hansen, et al., would
perform as well is trained on enough data.  In the data we presented, the simple
method developed by Hansen, et al., performed poorly regardless of the number of
reads used in training, which varied between hundreds of thousands to tens of
millions.  The methods presented by Li, et al., are more effective, yet involve
more parameters, and require more time to train, so that our method, while often
being more effective, also offers a significant simplification.

Secondly, it is asserted that we did not demonstrate that uniformity of read
coverage is increased ofter correcting for bias using our method.  Yet, in
section 3.2 we had presented a rigorous statistical analysis over a large and
diverse collection of data sets showing that this is indeed the case. This
section of the paper was evidently overlooked by the reviewer, as it received
no mention in this criticism.

Finally, the reviewer doubts the biological relevance of the bias correction
methods. Having demonstrated an increase in uniformity (section 3.2) and a
decrease in nucleotide frequency divergence (section 3.1), we believe a
compelling case was made that correcting for bias increases the accuracy of
quantification. In this revision we have supplied another line of evidence:
analysis showing that correlation between RNA-Seq and qRT-PCR is increased after
correcting for bias.

Under these circumstances we respectively request that the manuscript, in this
revised form, be reconsidered for publication.


\subsection*{Response to Reviewer 1}

\begin{quote}
(1) Page3, 2.2.1 mentions a correction for bias introduced by PCR amplification.
RNA sequencing methods are available where PCR amplification is not used.
Correcting for bias in these cases removes true duplication.  Could the authors
please comment on this?
\end{quote}

Because model complexity is penalized, a very sparse, or even empty model will
be trained on a data set without bias.  If PCR amplification is responsible for
a majority of the bias, and an RNA-Seq method without PCR amplification is used
producing largely bias-free data, then using the method to correct for bias
will have little if any effect.

We have included new data examining the FRT-Seq protocol of Mamanova, et. al.,
in which no PCR-amplification in performed during library preparation. We show
that bias is reduced, but not eliminated in this method, and that our method
accounts for this reduced bias by constructing a much sparser model, relative to
the other RNA-Seq data sets we have examined.


\subsection*{Response to Reviewer 2}

\begin{quote}
The ms presents an approach to estimating the effect of local sequence
context on the rate of seeing aligned reads at each position within a
transcript in RNA-Seq data. The method is inspired by Hansen et al.'s,
which simply determines this bias by comparing heptamer frequencies.

Compared to the brute force approach of Hansen, who estimated model
parameters (the density of a joint distribution) directly from
heptamer frequencies, here a Bayesian network approach is taken, which
allows for a sparser parameterisation. However, the cost for this are
a plethora of tuning parameters (e.g. the complexity penalty, data
sampling, $p_{max}$, $d_{max}$) and a greedy algorithm with apparently no
control of the quality of its solution.
\end{quote}

The method has precisely the following parameters,
\begin{enumerate}
\item The number of reads the method is trained on.
\item The size of the window surrounding read starts over which the model should
be trained.
\item The maximum in-degree (i.e., $p_{\text{max}}$)
\item The maximum edge distance (i.e., $d_{\text{max}}$).
\item The standard deviation at which background sequences are sampled.
\item The model complexity penalty.
\end{enumerate}

Parameters (1), (2), (3), and (4) are introduced only to reduce the CPU time
needed to train the algorithm. Each has the same very simple interpretation: a
larger value leads to solution that is no worse and potentially better, at the
cost of increased training time.  We mention these parameters in the manuscript
only for the sake of transparency.  They generally require no intervention or
tuning, being essentially an implementation detail. 

Parameters (5) and (6) are more important to the quality of the solution, yet we
have demonstrated, with no special tuning, the method is effective across a
diverse collection of data sets. In this revision we have included new data
showing explicitly that the quality of the solution is remarkably insensitive to
values of these two parameters.

It should also be noted that, while the brute-force method of Hansen, et. al.,
has fewer parameters, the other methods evaluated in the manuscript (the GLM and
MART methods of Li, et. al.), involve more parameters that the method we
have described. (For example training the MART method involves parameters
controlling the complexity penalty, the number of trees, the lower bound
threshold for read counts, in addition to others that are not described or
easily changed from the values divined by the authors.) Our method therefore
presents a simplification over competing methods.


\begin{quote}
To justify this cost, the
authors make the argument that their approach avoids the potential
overfitting of a brute force approach such as Hansen's. However, I am
not convinced: they evaluate both methods, theirs and Hansen's, on
'more than 600,000 reads', yet this seems unrealistic: current
datasets have hundreds of millions of reads, and that may just solve
the estimation precision problem in the simple brute force approach.

(I appreciate that the implementation of the brute force approach in
the Genominator package is ridiculously inefficient, but that can
easily be fixed by more competent programming.)
\end{quote}

We regret that we more not more explicit on this point. At least 600,000 reads
on each data set, but as many as 23 million were used. (As noted in the paper,
we did reimplement the Genominator method in order to do this.) The brute-force
method performed very poorly on every data set, suggesting that simply throwing
more data at a suboptimal solution will not adequately solve the problem. The
results as they were presented are unambiguous: the other methods perform far
better with far fewer reads.

Moreover, even if the brute-force method were effective if trained with many
hundreds of millions of reads, many (possibly most) existing data sets are not
this large, rendering it generally impractical.


\begin{quote}
The major weakness of the ms, in my view, is the choice of 'figures of
merit' that the authors made (Fig.1, Sections 3.1, 3.2). What I would
have expected to see are

1.) Plots closer to the data such as Supplementary Fig.5 of
reference [1]. That figure shows highly uneven coverage of reads
within exons.
- Does this unevenness go away with the proposed adjustment?
- Also, as is evident from that figure, for some of the experimental
protocols, some regions within highly expressed exons get vanishingly
small read coverage.  These are the really hard situations, and much
more worrying than instances where coverage just fluctuates a bit.
How does the proposed scaling adjustment of Section 2.1 work in these
places (i.e. when $s_{i}$ is very small)?
\end{quote}

During preparation we choose not to include a figure such as the one suggested
by the reviewer as we believed it would not be legitimately informative. It
would be far too easy to cherry-pick a minuscule genomic interval that makes
even the very shoddy method appear favorable.

Instead, we presented data (in Section 3.2) that answers the reviewers question
(i.e., ``does this unevenness go away with the proposed adjustment?'') in
rigorous and comprehensive manner. We implemented a Poisson regression metric
and performed a large cross-validation study proving the effectiveness of our
method at increasing uniformity quantitatively, across thousands of exons drawn
from a diverse group of data sets, rather than qualitatively across one exon.

In this revision, we have included a figure, as suggested, showing an example of
this increase in uniformity. Yet, we must stress, this figure represents a
minute, unreliable subset of the analysis that had previously been presented,
and is included only as an illustration.


\begin{quote}
2.) A 'bottom-line', biologically relevant figure of merit: E.g., does
the identification of differentially expressed genes improve? The
clustering of gene expression profiles? The identification of
transcript isoforms?
\end{quote}

In Section 3, we demonstrate, after correction, a significant improvement in
the two obvious anomalies associated with the issue of sequence bias:
non-uniform read coverage, and divergent nucleotide frequencies. We assess these
aspects in a statistically rigorous manner and with a large sample size. We
believe that this analysis presents a compelling case that bias correction is
worthwhile, and that the method we present is competitive with, and quite often
superior to existing methods.

In this revision, we have included more data, showing that in addition to the
improvements in uniformity and nucleotide frequencies, increased correlation
between RNA-Seq and qRT-PCR, after adjusting for bias in RNA-Seq. These results
are consistent with, and reinforce our previous results.


\begin{quote}
Minor points:
------------
1. The ms seems confused between classification and regression.
Reference [2] might be helpful to resolve some of that.
\end{quote}

The term ``classification'' was used in the context of noting that the algorithm
is adapted from one typically used in classification. It is correct to say that
the method here performs regression, and we have tried to clarify this language
in this revision.



\begin{quote}
2. The chosen Bayesian network is rather complicated, and a spectrum
of alternative methods exists between that and the brute force
approach of Hansen (say, lasso or svm regression). In absence of a
more meaningful benchmark, the choice of method seems arbitrary.
\end{quote}

Our motivation for adopting a Bayesian network model is the very natural
probabilistic interpretation that we have described in Section 2.1. We
disagree with the reviewer's opinion that Bayesian networks are inherently more
complicated than Lasso regression or support vector machines.  In this
application, the latter two methods would generate output that is arguably less
transparent and interpretable, and neither be significantly simpler to
implement, nor involve fewer parameters.

Naturally, it is possible that another model might perform as well or better
than the one presented here, yet it is unrealistic or impossible to prove that
this is not case. What we have demonstrated is a model that is well reasoned,
efficiently implemented, and effective at reducing sequence bias in RNA-Seq.



\end{document}


